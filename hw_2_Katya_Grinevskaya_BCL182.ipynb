{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Домашняя работа 2. Гриневская Катя, БКЛ182**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала импортирую всё необходимое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pymorphy2\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# устанавливаем mystem\n",
    "# ! pip install pymystem3\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install slovnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Using cached https://files.pythonhosted.org/packages/cd/19/902d1691c1963ab8c9a9578abc2d65c63aa1ecf4f8200143b5ef91ace6f5/flair-0.6.1-py3-none-any.whl\n",
      "Collecting konoha<5.0.0,>=4.0.0 (from flair)\n",
      "  Using cached https://files.pythonhosted.org/packages/ea/01/47358efec5396fc80f98273c42cbdfe7aab056252b07884ffcc0f118978f/konoha-4.6.2-py3-none-any.whl\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from flair) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from flair) (2.8.0)\n",
      "Collecting pytest>=5.3.2 (from flair)\n",
      "  Using cached https://files.pythonhosted.org/packages/d6/36/9e022b76a3ac440e1d750c64fa6152469f988efe0c568b945e396e2693b5/pytest-6.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: langdetect in c:\\users\\user\\anaconda3\\lib\\site-packages (from flair) (1.0.8)\n",
      "Collecting gdown (from flair)\n",
      "  Using cached https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting deprecated>=1.2.4 (from flair)\n",
      "  Using cached https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
      "Collecting transformers>=3.0.0 (from flair)\n",
      "  Using cached https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl\n",
      "Collecting torch>=1.1.0 (from flair)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Could not find a version that satisfies the requirement torch>=1.1.0 (from flair) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)\n",
      "ERROR: No matching distribution found for torch>=1.1.0 (from flair)\n"
     ]
    }
   ],
   "source": [
    "! pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Русский язык"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тексте написанном мной, содержатся следующие затруднительные для POS-теггинга моменты:\n",
    "1) аббревиатура\n",
    "2) солва с дефисом (предлоги, существительные, прилагательные)\n",
    "3) имена собственные\n",
    "4) сложные предлоги\n",
    "5) сленговые слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импортируем текст и теги и токенизируем через nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw_2_rus.txt', encoding='utf-8') as fh:\n",
    "    text = fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw_2_rus_tagged.txt', encoding='utf-8') as fh:\n",
    "    text_tagged = fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_commas = word_tokenize(text_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorph_pos = []\n",
    "for token in tokens:\n",
    "    if token not in \"\"\"!@#$%^&*()_+'?/.<>,|\\\"``''~№=-:;\"\"\":\n",
    "        anal = morph.parse(token)\n",
    "        pymorph_pos.append(anal[0].tag.POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pymorph_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pos = []\n",
    "for tag in pos_tags_commas:\n",
    "    if tag not in \"\"\"!@#$%^&*()_+'?/.<>,|\\\"``''~№=-:;\"\"\":\n",
    "        my_pos.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество тэгов совпадает с количеством токенов (проверила на всякий случай) - можно идти дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мне нужно привести тэги к единому формату - я буду приводить к своему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorph_stand_pos = []\n",
    "for tag in pymorph_pos:\n",
    "    if tag in 'NOUN':\n",
    "        pymorph_stand_pos.append('N')\n",
    "    elif (tag in 'ADJF') or (tag in 'ADJS') or (tag in 'COMP'):\n",
    "        pymorph_stand_pos.append('ADJ')\n",
    "    elif (tag in 'VERB') or (tag in 'INFN'):\n",
    "        pymorph_stand_pos.append('V')\n",
    "    elif tag in 'NUMR':\n",
    "        pymorph_stand_pos.append('NUMB')\n",
    "    elif (tag in 'PRTF') or (tag in 'PRTS'):\n",
    "        pymorph_stand_pos.append('PRT')\n",
    "    elif (tag in 'ADVB') or (tag in 'PRED'):\n",
    "        pymorph_stand_pos.append('AVD')\n",
    "    elif tag in 'NPRO':\n",
    "        pymorph_stand_pos.append('PRON')\n",
    "    else:\n",
    "        pymorph_stand_pos.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pymorph_stand_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8646\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.4f' % accuracy_score(pymorph_stand_pos, my_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) теперь mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_pos = []\n",
    "for d in m.analyze(text):\n",
    "    if 'analysis' in d.keys():\n",
    "        mystem_pos.append(d['analysis'][0]['gr'].split(',')[0].split('=')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_stand_pos = []\n",
    "for tag in mystem_pos:\n",
    "    if (tag == 'NUM') or (tag == 'ANUM'):\n",
    "        mystem_stand_pos.append('NUMB')\n",
    "    elif tag == 'PR':\n",
    "        mystem_stand_pos.append('PREP')\n",
    "    elif tag == 'S':\n",
    "        mystem_stand_pos.append('N')\n",
    "    elif (tag == 'SPRO') or (tag == 'APRO') or (tag == 'ADVPRO'):\n",
    "        mystem_stand_pos.append('PRON')\n",
    "    elif tag == 'A':\n",
    "        mystem_stand_pos.append('ADJ')\n",
    "    elif tag == 'PART':\n",
    "        mystem_stand_pos.append('PRCL')\n",
    "    else:\n",
    "        mystem_stand_pos.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.4f' % accuracy_score(mystem_stand_pos, my_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_pos = []\n",
    "for obj in doc.tokens:\n",
    "    if obj.pos != 'PUNCT':\n",
    "        natasha_pos.append(obj.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(natasha_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Количество тэгов совпадает => идём дальше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_stand_pos = []\n",
    "for tag in natasha_pos:\n",
    "    if (tag == 'NOUN') or (tag == 'PROPN'):\n",
    "        natasha_stand_pos.append('N')\n",
    "    elif tag == 'ADP':\n",
    "        natasha_stand_pos.append('PREP')\n",
    "    elif tag == 'NUM':\n",
    "        natasha_stand_pos.append('NUMB')\n",
    "    elif tag == 'VERB':\n",
    "        natasha_stand_pos.append('V')\n",
    "    elif tag.endswith('CONJ'):\n",
    "        natasha_stand_pos.append('CONJ')\n",
    "    elif tag == 'DET':\n",
    "        natasha_stand_pos.append('PRON')\n",
    "    else:\n",
    "        natasha_stand_pos.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.4f' % accuracy_score(natasha_stand_pos, my_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Английский язык"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тексте написанном мной, содержатся следующие затруднительные для POS-теггинга моменты:\n",
    "1) слова с дефисом\n",
    "2) сленг/обсценная лексика\n",
    "3) имена собственные\n",
    "4) одинаковые словоформы для разных частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импортируем текст и теги и токенизируем через nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw_2_eng.txt', encoding='utf-8') as fh:\n",
    "    text = fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw_2_eng_tagged.txt', encoding='utf-8') as fh:\n",
    "    text_tagged = fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dirty = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in range(len(tokens_dirty)):\n",
    "    if tokens_dirty[i] not in \"\"\"!@#$%^&*()_+'?/.<>,|\\\"``''~№=-:;\"\"\":\n",
    "        tokens.append(tokens_dirty[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags_commas = word_tokenize(text_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pos = []\n",
    "for tag in pos_tags_commas:\n",
    "    if tag not in \"\"\"!@#$%^&*()_+'?/.<>,|\\\"``''~№=-:;\"\"\":\n",
    "        my_pos.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_pos = []\n",
    "for tag in nltk.pos_tag(tokens, tagset='universal'):\n",
    "    if tag[1] not in \"\"\"!@#$%^&*()_+'?/.<>,|\\\"``''~№=-:;\"\"\":\n",
    "        nltk_pos.append(tag[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё сошлось, ура! Стандартизируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stand_pos = []\n",
    "for tag in nltk_pos:\n",
    "    if tag == 'VERB':\n",
    "        nltk_stand_pos.append('V')\n",
    "    elif tag == 'NOUN':\n",
    "        nltk_stand_pos.append('N')\n",
    "    elif (tag == 'ADP') or (tag == 'PRT'):\n",
    "        nltk_stand_pos.append('PREP')\n",
    "    elif tag == 'NUM':\n",
    "        nltk_stand_pos.append('NUMB')\n",
    "    else:\n",
    "        nltk_stand_pos.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8793\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.4f' % accuracy_score(nltk_stand_pos, my_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy токенизирует слова с дефисом как 2 слова, поэтому тут добавляю специальное правило"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.char_classes import ALPHA, ALPHA_LOWER, ALPHA_UPPER\n",
    "from spacy.lang.char_classes import CONCAT_QUOTES, LIST_ELLIPSES, LIST_ICONS\n",
    "from spacy.util import compile_infix_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "infixes = (\n",
    "    LIST_ELLIPSES\n",
    "    + LIST_ICONS\n",
    "    + [\n",
    "        r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\",\n",
    "        r\"(?<=[{al}{q}])\\.(?=[{au}{q}])\".format(\n",
    "            al=ALPHA_LOWER, au=ALPHA_UPPER, q=CONCAT_QUOTES\n",
    "        ),\n",
    "        r\"(?<=[{a}]),(?=[{a}])\".format(a=ALPHA),\n",
    "        # EDIT: commented out regex that splits on hyphens between letters:\n",
    "        #r\"(?<=[{a}])(?:{h})(?=[{a}])\".format(a=ALPHA, h=HYPHENS),\n",
    "        r\"(?<=[{a}0-9])[:<>=/](?=[{a}])\".format(a=ALPHA),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "infix_re = compile_infix_regex(infixes)\n",
    "nlp.tokenizer.infix_finditer = infix_re.finditer\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get   AUX\n",
      "the   DET\n",
      "hell   NOUN\n",
      "out   SCONJ\n",
      "of   ADP\n",
      "here   ADV\n",
      "said   VERB\n",
      "Jenny   PROPN\n",
      "to   ADP\n",
      "her   DET\n",
      "well-paid   ADJ\n",
      "colleague   NOUN\n",
      "on   ADP\n",
      "Wednesday   PROPN\n",
      "Fuck   VERB\n",
      "off   ADP\n",
      "answered   VERB\n",
      "Peter   PROPN\n",
      "You   PRON\n",
      "'ll   VERB\n",
      "never   ADV\n",
      "know   VERB\n",
      "how   ADV\n",
      "much   ADJ\n",
      "pain   NOUN\n",
      "I   PRON\n",
      "feel   VERB\n",
      "about   ADP\n",
      "it   PRON\n",
      "You   PRON\n",
      "are   AUX\n",
      "psycho   ADJ\n",
      "All   DET\n",
      "that   DET\n",
      "glisters   NOUN\n",
      "is   AUX\n",
      "not   PART\n",
      "gold   ADJ\n",
      "This   DET\n",
      "job   NOUN\n",
      "will   VERB\n",
      "put   VERB\n",
      "gold   NOUN\n",
      "right   ADV\n",
      "in   ADP\n",
      "my   DET\n",
      "pocket   NOUN\n",
      "while   SCONJ\n",
      "you   PRON\n",
      "think   VERB\n",
      "it   PRON\n",
      "'s   AUX\n",
      "shit   NOUN\n",
      "Boss   NOUN\n",
      "said   VERB\n",
      "I   PRON\n",
      "can   VERB\n",
      "increase   VERB\n",
      "my   DET\n",
      "income   NOUN\n",
      "twice   ADV\n",
      "I   PRON\n",
      "am   AUX\n",
      "not   PART\n",
      "going   VERB\n",
      "to   PART\n",
      "pay   VERB\n",
      "attention   NOUN\n",
      "to   ADP\n",
      "this   DET\n",
      "Well   INTJ\n",
      "I   PRON\n",
      "am   AUX\n",
      "not   PART\n",
      "afraid   ADJ\n",
      "I   PRON\n",
      "have   AUX\n",
      "spent   VERB\n",
      "eight   NUM\n",
      "hard-working   ADJ\n",
      "years   NOUN\n",
      "here   ADV\n",
      "and   CCONJ\n",
      "I   PRON\n",
      "know   VERB\n",
      "I   PRON\n",
      "am   AUX\n",
      "going   VERB\n",
      "to   PART\n",
      "find   VERB\n",
      "better   ADJ\n",
      "opportunity   NOUN\n",
      "Perhaps   ADV\n",
      "I   PRON\n",
      "can   VERB\n",
      "never   ADV\n",
      "go   VERB\n",
      "back   ADV\n",
      "said   VERB\n",
      "Peter   PROPN\n",
      "and   CCONJ\n",
      "left   VERB\n",
      "the   DET\n",
      "room   NOUN\n",
      "Jenny   PROPN\n",
      "'s   PART\n",
      "record   NOUN\n",
      "of   ADP\n",
      "Peter   PROPN\n",
      "'s   PART\n",
      "speech   NOUN\n",
      "will   VERB\n",
      "show   VERB\n",
      "who   PRON\n",
      "is   AUX\n",
      "right   ADJ\n"
     ]
    }
   ],
   "source": [
    "spacy_pos = []\n",
    "for i, s in enumerate(doc.sents):\n",
    "    for t in s:\n",
    "        if (t.pos_ != 'PUNCT') and (t.pos_ != 'SPACE'):\n",
    "            print(t.text, ' ', t.pos_)\n",
    "            spacy_pos.append(t.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё сошлось, ура! Стандартизируем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stand_pos = []\n",
    "for tag in spacy_pos:\n",
    "    if (tag == 'AUX') or (tag == 'VERB'):\n",
    "        spacy_stand_pos.append('V')\n",
    "    elif (tag == 'PROPN') or (tag == 'NOUN'):\n",
    "        spacy_stand_pos.append('N')\n",
    "    elif tag.endswith('CONJ'):\n",
    "        spacy_stand_pos.append('CONJ')\n",
    "    elif tag == 'NUM':\n",
    "        spacy_stand_pos.append('NUMB')\n",
    "    elif (tag == 'ADP') or (tag == 'PART'):\n",
    "        spacy_stand_pos.append('PREP')\n",
    "    else:\n",
    "        spacy_stand_pos.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8534\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %.4f' % accuracy_score(spacy_stand_pos, my_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-56-b115eb0cf751>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-56-b115eb0cf751>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    tagger = SequenceTagger.load('upos)\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "\n",
    "\n",
    "# initialize sentence splitter\n",
    "splitter = SegtokSentenceSplitter()\n",
    "\n",
    "# use splitter to split text into list of sentences\n",
    "sentences = splitter.split(text)\n",
    "\n",
    "# predict tags for sentences\n",
    "tagger = SequenceTagger.load('upos')\n",
    "tagger.predict(sentences)\n",
    "\n",
    "# iterate through sentences and print predicted labels\n",
    "flair_pos = []\n",
    "for sentence in sentences:\n",
    "    tagged_sent = sentence.to_tagged_string()\n",
    "    for tag in re.findall(r'<\\w+>', tagged_sent):\n",
    "      if tag != '<PUNCT>':\n",
    "        flair_pos.append(tag.replace('<', '',).replace('>', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flair_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё сошлось, ура! Стандартизируем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_stand_pos = []\n",
    "for tag in flair_pos:\n",
    "    if tag == 'VERB':\n",
    "        flair_stand_pos.append('V')\n",
    "    elif tag == 'NOUN':\n",
    "        flair_stand_pos.append('N')\n",
    "    elif (tag == 'ADP') or (tag == 'PRT'):\n",
    "        flair_stand_pos.append('PREP')\n",
    "    elif tag == 'NUM':\n",
    "        flair_stand_pos.append('NUMB')\n",
    "    else:\n",
    "        flair_stand_pos.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: %.4f' % accuracy_score(flair_stand_pos, my_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возвращаемся к русскому языку. Берём лучший теггер для русского языка: у меня это mystem. Выделим 3 вида синтаксических групп: 1) не + прилагательное/наречие 2) не + глагол + __; 3) прил+сущ. Выделим такие n-граммы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взяла именно такие группы, потому что рассчитываю на частые словосочетания в отзывах: \n",
    "1. _не интересный, не впечатляющий, не приятный, не захватывающий_ в отрицательных отзывах;\n",
    "2. _не показался интересным/весёлым/..., не понравился фильм/сюжет/состав/..., не привлёк внимание, не уважаю такое_ чтобы знать, что после глагола;\n",
    "3. _плохой/отстойный/ужасный фильм_ и _хороший/потрясный/захватывающий фильм_ не будут сливаться в одно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(otz):\n",
    "    ne_ad = [] # здесь будут не+прилагательные\n",
    "    ne_verb_sm = [] # здесь не+глагол+что-то\n",
    "    adj_n = [] # здесь прил+сущ\n",
    "    a = m.analyze(otz.lower())\n",
    "    for i in range(len(a)):\n",
    "        if 'analysis' in a[i].keys():\n",
    "            if (a[i]['text'] == 'не') and (a[i+2]['analysis'][0]['gr'].split(',')[0].split('=')[0] == 'A'):\n",
    "                ne_ad.append('не'+' '+a[i+2]['text'])\n",
    "            elif (a[i]['text'] == 'не') and (a[i+2]['analysis'][0]['gr'].split(',')[0].split('=')[0] == 'V'):\n",
    "                if (len(a)-i) > 4:\n",
    "                    ne_verb_sm.append('не' + ' ' + a[i+2]['text'] + ' ' +  a[i+4]['text'])\n",
    "                else:\n",
    "                    ne_verb_sm.append('не' + ' ' + a[i+2]['text'])\n",
    "            elif ((a[i]['analysis'][0]['gr'].split(',')[0].split('=')[0] == 'A') and\n",
    "                  (a[i+2]['analysis'][0]['gr'].split(',')[0].split('=')[0] == 'S')):\n",
    "                if (len(a)-i) > 2:\n",
    "                    adj_n.append(a[i]['text'] + ' ' + a[i+2]['text'])\n",
    "                else:\n",
    "                    continue\n",
    "    return ne_ad, ne_verb_sm, adj_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "otz = \"\"\"этот парень вообще не красивый и совсем Не умный и я не люблю кошек и грозных собак\"\"\"\n",
    "ne_ad, ne_verb_sm, adj_n = ngrams(otz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['не красивый', 'не умный']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['не люблю кошек']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_verb_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['грозных собак']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
