{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw_1_Katya_Grinevskaya_BCL182.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "482cxkliguke"
      },
      "source": [
        "## **Домашняя работа 1. Гриневская Катя, БКЛ182**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MZ2hqSqg8ug"
      },
      "source": [
        "Сначала импортирую всё необходимое"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFBgQQwFhA3B"
      },
      "source": [
        "import urllib.request\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import *\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idc66AdrZNBP",
        "outputId": "d0825b67-50df-4c0a-a5b7-173edf759f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilNFCbQOhHoV",
        "outputId": "59838442-9349-4d81-f66a-38d4341e0c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "! pip install pymorphy2\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.9MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/9b/358faaff410f65a4ad159275e897b5956dcb20576c5b8e764b971c1634d7/pymorphy2_dicts_ru-2.4.404381.4453942-py2.py3-none-any.whl (8.0MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0MB 4.2MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.404381.4453942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEngFGIykfZ3",
        "outputId": "8084db85-b2bf-4ffe-df6d-f7594a88c48f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "russian_stopwords = stopwords.words(\"russian\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0vGTZmihLLN"
      },
      "source": [
        "Сохраняю две ссылки на положительные и отрицательные отзывы (сразу по 50 штук на странице) на фильм \"Чёрная пантера\" с Кинопоиска. По 45 отзывов я возьму для составления тонального словаря, по 5 оставлю для проверки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvRgpdzqhRyy"
      },
      "source": [
        "url_p = 'https://www.kinopoisk.ru/film/623250/reviews/ord/rating/status/good/perpage/50'\n",
        "url_n = 'https://www.kinopoisk.ru/film/623250/reviews/ord/rating/status/bad/perpage/50'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwYZ-aY_iGtR"
      },
      "source": [
        "Функция, которая парсит html-страничку и оставляет только список отзывов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1E0aohsiXkK"
      },
      "source": [
        "def parser(url):\n",
        "  page = urllib.request.urlopen(url)\n",
        "  text = page.read().decode('utf-8')\n",
        "  soup = BeautifulSoup(text, 'html.parser')\n",
        "  recs = soup.find_all('div', {'class': 'brand_words'}) \n",
        "  otzs = []\n",
        "  # очищаем от остатков html\n",
        "  for res in recs:\n",
        "    otzs.append(res.text)\n",
        "  clean_otzs = []\n",
        "  for otz in otzs:\n",
        "    clean_otz = re.sub(r'[\\n\\t\\r]', ' ', otz)\n",
        "    clean_otzs.append(clean_otz)\n",
        "  return clean_otzs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRhzOm0kvfPU"
      },
      "source": [
        "clean_otzs_p_all = parser(url_p)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJTeNa4MvtaK"
      },
      "source": [
        "clean_otzs_n_all = parser(url_n)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXwDgWftX5lT"
      },
      "source": [
        "clean_otzs_p, clean_otzs_p_test = clean_otzs_p_all[:45], clean_otzs_p_all[45:]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9mQNBCCYABq"
      },
      "source": [
        "clean_otzs_n, clean_otzs_n_test = clean_otzs_n_all[:45], clean_otzs_n_all[45:]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx-LJmmwwfj8"
      },
      "source": [
        "clean_otzs_test = clean_otzs_p_test + clean_otzs_n_test"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS_DTBCGRa59"
      },
      "source": [
        "Проверяю, все ли отзывы скачались"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2XMTuCPdc1u",
        "outputId": "982193b5-71ba-40aa-d94e-e29d056f55aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(clean_otzs_p)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lci8j9ScmIJ3",
        "outputId": "53b228b4-469b-4391-e04c-6efec141e5f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(clean_otzs_n)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRnPwwzQwoDk",
        "outputId": "271a20f0-e241-49ce-9cdc-282489076636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(clean_otzs_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k1xcmYjjh4G"
      },
      "source": [
        "Токенизиурем, лемматизируем, избавляемся от стоп-слов, возвращаем список токенов поданного отзыва"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrmjjcaEjoRq"
      },
      "source": [
        "def to_list(otz):\n",
        "  otz_tokens = []\n",
        "  numb = re.compile(r'\\d')\n",
        "  tokens = nltk.word_tokenize(otz)\n",
        "  for token in tokens:\n",
        "    if token not in \"\"\"!@#$%^&*{}()_+`’`'?/--–.<>,|«»\\\"~№=-:;\"\"\":\n",
        "      # проверяем, чтобы не было числом\n",
        "      # не использую .isalpha(), потому что хочу оставить слова с дефисом\n",
        "      if len(numb.findall(token)) == 0:\n",
        "        for char in token:\n",
        "          if char in \"\"\"!@#$%^&*{}«»()`_+'?/.<>,|\\\"~№=:;\"\"\":\n",
        "            token = token.replace(char, '')\n",
        "        anal = morph.parse(token)\n",
        "        token_lemm = anal[0].normal_form\n",
        "        if token_lemm not in russian_stopwords:\n",
        "          otz_tokens.append(token_lemm)\n",
        "  # freqlist = dict(Counter(rez_tokens))\n",
        "  return otz_tokens"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2-wsnXkC7xn"
      },
      "source": [
        "Теперь объединяем токены всех отзывов в один список, делаем из него словарь с частотностями, убираем слова с частотностью не больше 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31CV53b14KfI"
      },
      "source": [
        "def to_dict(clean_otzs):\n",
        "  rez_tokens = []\n",
        "  rez_tokens_d = {}\n",
        "  for otz in clean_otzs:\n",
        "    rez_tokens += to_list(otz)\n",
        "  freqlist = dict(Counter(rez_tokens))\n",
        "  for key, value in freqlist.items():\n",
        "    if value > 3:\n",
        "      rez_tokens_d[key] = value\n",
        "  return rez_tokens_d"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUiVelO8ZA4O"
      },
      "source": [
        "rez_tokens_d_p = to_dict(clean_otzs_p)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7yqbsJRZR4O"
      },
      "source": [
        "rez_tokens_d_n = to_dict(clean_otzs_n)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6jeLdWumKib"
      },
      "source": [
        "Оставляем в одном из полученных множеств те слова, которые встречаются только в положительных отзывах, а в другом - встречающиеся только в отрицательных. Получаем словарь {'pos' : {'token': frequency, ...}, 'neg': {'token': frequency, ...}}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEDu_EHrmdNb"
      },
      "source": [
        "def clean_dicts(dict_pos, dict_neg):\n",
        "  dict_pos_uniq = {}\n",
        "  dict_neg_uniq = {}\n",
        "  for key in dict_neg.keys():\n",
        "    if key not in dict_pos.keys():\n",
        "      dict_neg_uniq[key] = dict_neg[key]\n",
        "  for key in dict_pos.keys():\n",
        "    if key not in dict_neg.keys():\n",
        "      dict_pos_uniq[key] = dict_pos[key]\n",
        "  freq_dict = {'pos': dict_pos_uniq, 'neg' : dict_neg_uniq}\n",
        "  return freq_dict"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WSb0dFecChB"
      },
      "source": [
        "freq_dict = clean_dicts(rez_tokens_d_p, rez_tokens_d_n)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx6zaGOqNSVl"
      },
      "source": [
        "Теперь у нас есть тональный словарь. Создаём функцию, которая проверяет, сколько общих слов у проверяемого отзыва со словарём из положительных отзывов, а сколько - со словарём из отрицательных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbMRX-knNwv0"
      },
      "source": [
        "def tone_detect(freq_dict, test_text):\n",
        "  counts = Counter()\n",
        "  test_text_tokens = to_list(test_text)\n",
        "  for tone, freqdict in freq_dict.items():\n",
        "    for word in test_text_tokens:\n",
        "      if word in freqdict.keys():\n",
        "        counts[tone] += freqdict[word]\n",
        "  return counts.most_common()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQwj2QwIFGiA"
      },
      "source": [
        "Прогоняем на всех 10 отзывах, оставленных для проверки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR8HCuRNN9jN",
        "outputId": "cad5163c-01ed-4625-fb12-9ae901169f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "for text in clean_otzs_test:\n",
        "  print(tone_detect(freq_dict, text)[0][0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos\n",
            "pos\n",
            "pos\n",
            "pos\n",
            "pos\n",
            "neg\n",
            "pos\n",
            "neg\n",
            "neg\n",
            "pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsOZM0YYOGs7"
      },
      "source": [
        "Итого: из 10 текстов на проверку 5 первых были позитивными, 5 вторых - негативными. Все позитивные отзывы правильно определены, а негативные - не все. Я думаю, что это связано с тем, что в негативных отзывах часто могут упоминаться словосочетания типа \"не впечатлило\", \"не то чтобы прелестный\", когда автор использует редкое позитивное слово с отрицательными единицами, которые уходят засчёт стоп-слов или удаления совпадающих элементов в тональных словарях."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie_0NxVXO0Yc"
      },
      "source": [
        "Теперь посчитаем accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqPJCzm8M0yI",
        "outputId": "774be5dc-1954-41a1-da25-90adfa9b81a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDRxUhp9M5Cr"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG0s1M-A3Y4c",
        "outputId": "9a58e2a5-d0df-4e8f-c5de-a06d1e85612d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "results = []\n",
        "gold = ['pos', 'pos', 'pos', 'pos', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg']\n",
        "for text in clean_otzs_test:\n",
        "  results.append(tone_detect(freq_dict, text)[0][0])\n",
        "print('RESULTS:')\n",
        "print('Accuracy: %.4f' % accuracy_score(results, gold))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RESULTS:\n",
            "Accuracy: 0.8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EejElqQ7O_yJ"
      },
      "source": [
        "Улучшить программу можно следующими способами: \n",
        "\n",
        "1.   выбрать другой способ обкачки сайта с отзывами, чтобы можно было за раз получить большее количество отзывов, чтобы увеличить объём тональных словарей\n",
        "2.   придумать более точный вариант сравнения частотностей слов с частотностями из тональных словарей: например, не просто складывать количество упоминаний, а складывать именно частотности (количество упоминаний делённое на объём текста в случае с отзывом на проверку или на сумму объёмов текстов в случае с тональными словарями)\n",
        "3. ещё можно получше чистить от стоп-слов и шума (например, загрузить откуда-нибудь более полный список стоп-слов для русского языка, хотя они при применении п.1 должны уйти при взаимоисключении элементов из двух тональных словарей)"
      ]
    }
  ]
}